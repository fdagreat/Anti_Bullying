{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Project - Helpline of all sorts</b><br><br>\n",
    "Implimenting LSTM model to predict what the concern is about:<br>\n",
    "0 - work life harassment and stress<br>\n",
    "1 - school bullying related complaints<br>\n",
    "2 - sexual harassment related complaints<br>\n",
    "3 - whistleblowing<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f727d4394867>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "numpy.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b>Converting input sentences into vector word representation, leveraged pre-trained 100-D GloVe embedding.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '../Desktop/datasets_and_embeddings/glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "# f = open(os.path.join(data_dir, 'glove.6B.100d.txt'))\n",
    "f = open(data_dir, encoding=\"utf8\")\n",
    "i = 0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    word_to_index[word] = i\n",
    "    index_to_word[i] = word\n",
    "    embedding = numpy.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = embedding\n",
    "    i+=1\n",
    "f.close()\n",
    "\n",
    "# for testing purpose to make sure we are getting correct word vectors\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(\"sample embedding of word 'bangalore' is\", embeddings_index['bangalore'])\n",
    "word = \"cucumber\"\n",
    "index = 50000\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index+1) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b>loading the training datsaset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = '../Desktop/datasets_and_embeddings/helpline_datasets.csv'\n",
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "with open(filename,'r',encoding=\"utf-8\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        X_train_list.append(row[0])\n",
    "        Y_train_list.append(row[1])\n",
    "            \n",
    "X_train = numpy.array(X_train_list) # converting list into 1-D vector representation   \n",
    "Y_train = numpy.array(Y_train_list) # converting list into 1-D vector representation\n",
    "num_training_sets = 0\n",
    "try:\n",
    "    num_training_sets = numpy.prod(X_train.shape)\n",
    "    print(\"total datasets in training: \",num_training_sets)\n",
    "    if num_training_sets != numpy.prod(X_train.shape):\n",
    "        raise ValueError\n",
    "except ValueError:\n",
    "    print(\"dimensions of datasets won't match, verify once!!!!!\")\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "index = 9\n",
    "print(\"sample of training set in 9th index: \",X_train[index], Y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><b>Converting array of sentences into array of indices so to feed embeddings() function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0] #number of training examples\n",
    "    X_indices = numpy.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i,j] = word_to_index[w]\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = numpy.array([\"for the past several days I'm suffering from a lot of mental tension due to ongoing audits in office late night\",\n",
    "                 \"I got bullied yesterday from a bunch of senior guys during interval\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_length = 21)\n",
    "print(\"X1=\",X1)\n",
    "print(\"X1_indices=\",X1_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
